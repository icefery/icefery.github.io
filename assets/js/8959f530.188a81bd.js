"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[5516],{3905:(e,n,a)=>{a.d(n,{Zo:()=>s,kt:()=>N});var t=a(7294);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function p(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function l(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var i=t.createContext({}),d=function(e){var n=t.useContext(i),a=n;return e&&(a="function"==typeof e?e(n):p(p({},n),e)),a},s=function(e){var n=d(e.components);return t.createElement(i.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),m=d(a),c=r,N=m["".concat(i,".").concat(c)]||m[c]||u[c]||o;return a?t.createElement(N,p(p({ref:n},s),{},{components:a})):t.createElement(N,p({ref:n},s))}));function N(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,p=new Array(o);p[0]=c;var l={};for(var i in n)hasOwnProperty.call(n,i)&&(l[i]=n[i]);l.originalType=e,l[m]="string"==typeof e?e:r,p[1]=l;for(var d=2;d<o;d++)p[d]=a[d];return t.createElement.apply(null,p)}return t.createElement.apply(null,a)}c.displayName="MDXCreateElement"},1964:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>i,contentTitle:()=>p,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var t=a(7462),r=(a(7294),a(3905));const o={},p=void 0,l={unversionedId:"big-data/hadoop/hadoop-3.3.1 \u5b89\u88c5",id:"big-data/hadoop/hadoop-3.3.1 \u5b89\u88c5",title:"hadoop-3.3.1 \u5b89\u88c5",description:"\u73af\u5883\u8bf4\u660e",source:"@site/docs/big-data/hadoop/hadoop-3.3.1 \u5b89\u88c5.md",sourceDirName:"big-data/hadoop",slug:"/big-data/hadoop/hadoop-3.3.1 \u5b89\u88c5",permalink:"/docs/big-data/hadoop/hadoop-3.3.1 \u5b89\u88c5",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"defaultSidebar",previous:{title:"Hadoop",permalink:"/docs/big-data/hadoop/"},next:{title:"HDFS",permalink:"/docs/big-data/hadoop/hdfs/"}},i={},d=[{value:"\u73af\u5883\u8bf4\u660e",id:"\u73af\u5883\u8bf4\u660e",level:3},{value:"\u53c2\u8003",id:"\u53c2\u8003",level:3},{value:"Preudo-Distributed \u6a21\u5f0f",id:"preudo-distributed-\u6a21\u5f0f",level:3},{value:"\u914d\u7f6e\u6587\u4ef6",id:"\u914d\u7f6e\u6587\u4ef6",level:4},{value:"\u683c\u5f0f\u5316 NameNode",id:"\u683c\u5f0f\u5316-namenode",level:4},{value:"\u542f\u52a8",id:"\u542f\u52a8",level:4},{value:"Fully-Distributed \u6a21\u5f0f",id:"fully-distributed-\u6a21\u5f0f",level:3},{value:"\u96c6\u7fa4\u89c4\u5212",id:"\u96c6\u7fa4\u89c4\u5212",level:4},{value:"\u914d\u7f6e\u6587\u4ef6",id:"\u914d\u7f6e\u6587\u4ef6-1",level:4},{value:"\u5206\u53d1\u914d\u7f6e",id:"\u5206\u53d1\u914d\u7f6e",level:4},{value:"\u683c\u5f0f\u5316 NameNode",id:"\u683c\u5f0f\u5316-namenode-1",level:4},{value:"\u542f\u52a8",id:"\u542f\u52a8-1",level:4},{value:"\u9a8c\u8bc1",id:"\u9a8c\u8bc1",level:4},{value:"High-Availability \u6a21\u5f0f",id:"high-availability-\u6a21\u5f0f",level:3},{value:"\u96c6\u7fa4\u89c4\u5212",id:"\u96c6\u7fa4\u89c4\u5212-1",level:4},{value:"\u914d\u7f6e\u6587\u4ef6",id:"\u914d\u7f6e\u6587\u4ef6-2",level:4},{value:"\u5206\u53d1\u914d\u7f6e",id:"\u5206\u53d1\u914d\u7f6e-1",level:4},{value:"\u542f\u52a8 JournalNode",id:"\u542f\u52a8-journalnode",level:4},{value:"\u683c\u5f0f\u5316 NameNode",id:"\u683c\u5f0f\u5316-namenode-2",level:4},{value:"\u683c\u5f0f\u5316 ZK",id:"\u683c\u5f0f\u5316-zk",level:4},{value:"\u542f\u52a8",id:"\u542f\u52a8-2",level:4},{value:"HA \u72b6\u6001",id:"ha-\u72b6\u6001",level:4},{value:"WordCount",id:"wordcount",level:2},{value:"\u793a\u4f8b",id:"\u793a\u4f8b",level:4},{value:"HDFS \u5ba2\u6237\u7aef",id:"hdfs-\u5ba2\u6237\u7aef",level:3},{value:"MapReduce \u7a0b\u5e8f",id:"mapreduce-\u7a0b\u5e8f",level:3}],s={toc:d};function m(e){let{components:n,...a}=e;return(0,r.kt)("wrapper",(0,t.Z)({},s,a,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h3",{id:"\u73af\u5883\u8bf4\u660e"},"\u73af\u5883\u8bf4\u660e"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"ubuntu-20.04")),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u542f\u7528 ",(0,r.kt)("inlineCode",{parentName:"p"},"root")," \u7528\u6237 SSH")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"SSH \u514d\u5bc6\u767b\u5f55")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"jdk-8")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("inlineCode",{parentName:"p"},"hadoop-3.3.1")," \u7248\u672c\u5df2\u7ecf\u652f\u6301 ",(0,r.kt)("inlineCode",{parentName:"p"},"jdk-11")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"zookeeper-3.7")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},"\u5730\u5740\uff1a",(0,r.kt)("inlineCode",{parentName:"p"},"win10:2181,win10:2182,win10:2183"))))),(0,r.kt)("h3",{id:"\u53c2\u8003"},"\u53c2\u8003"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.zhihu.com/question/264588465"},"hadoop \u4e3a\u4ec0\u4e48\u9700\u8981\u5728 hadoop-env.sh \u91cd\u65b0\u914d\u7f6e JAVA_HOME\uff1f")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://stackoverflow.com/questions/48129029/hdfs-namenode-user-hdfs-datanode-user-hdfs-secondarynamenode-user-not-defined"},"HDFS_NAMENODE_USER, HDFS_DATANODE_USER & HDFS_SECONDARYNAMENODE_USER not defined")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://blog.csdn.net/qq_22310551/article/details/85700978"},"hadoop ha \u6a21\u5f0f\u4e0b\uff0ckill active \u7684 namenode \u8282\u70b9\u540e\uff0cstandby \u7684 namenode \u8282\u70b9\u6ca1\u80fd\u81ea\u52a8\u542f\u52a8")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://kontext.tech/column/hadoop/447/install-hadoop-330-on-windows-10-step-by-step-guide"},"Install Hadoop 3.3.0 on Windows 10 Step by Step Guide")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://blog.csdn.net/vaf714/article/details/82996860"},"\u5916\u7f51\u65e0\u6cd5\u8bbf\u95ee HDFS \u89e3\u51b3\u65b9\u6cd5"))),(0,r.kt)("h3",{id:"preudo-distributed-\u6a21\u5f0f"},"Preudo-Distributed \u6a21\u5f0f"),(0,r.kt)("h4",{id:"\u914d\u7f6e\u6587\u4ef6"},"\u914d\u7f6e\u6587\u4ef6"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/hadoop-env.sh")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"export JAVA_HOME=/opt/jdk-8\nexport HADOOP_HOME=/opt/hadoop-3.3.1\nexport HDFS_NAMENODE_USER=root\nexport HDFS_SECONDARYNAMENODE_USER=root\nexport HDFS_DATANODE_USER=root\nexport YARN_RESOURCEMANAGER_USER=root\nexport YARN_NODEMANAGER_USER=root\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/workers")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"node101\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/core-site.xml")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("inlineCode",{parentName:"p"},"hadoop.tmp.dir")," \u4e0d\u80fd\u5f15\u7528\u73af\u5883\u53d8\u91cf")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://node101:9000</value>\n    </property>\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>/opt/hadoop-3.3.1/data</value>\n    </property>\n    <property>\n        <name>hadoop.http.staticuser.user</name>\n        <value>root</value>\n    </property>\n    <property>\n        <name>hadoop.proxyuser.root.hosts</name>\n        <value>*</value>\n    </property>\n    <property>\n        <name>hadoop.proxyuser.root.groups</name>\n        <value>*</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/hdfs-site.xml")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("inlineCode",{parentName:"p"},"dfs.replication")," \u9ed8\u8ba4\u4e3a 3")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>dfs.client.use.datanode.hostname</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/yarn-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n    <property>\n        <name>yarn.log-aggregation-enable</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>yarn.log-aggregation.retain-seconds</name>\n        <value>604800</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/mapred-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>yarn.app.mapreduce.am.env</name>\n        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n    </property>\n    <property>\n        <name>mapreduce.map.env</name>\n        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n    </property>\n    <property>\n        <name>mapreduce.reduce.env</name>\n        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n    </property>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\n')))),(0,r.kt)("h4",{id:"\u683c\u5f0f\u5316-namenode"},"\u683c\u5f0f\u5316 NameNode"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd $HADOOP_HOME\nbin/hdfs namenode -format\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"\u5982\u9700\u91cd\u65b0\u683c\u5f0f\u5316\uff0c\u9700\u5148\u5220\u9664 ",(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/data/dfs/name")," \u76ee\u5f55")),(0,r.kt)("h4",{id:"\u542f\u52a8"},"\u542f\u52a8"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd $HADOOP_HOME\nsbin/start-dfs.sh\nsbin/start-yarn.sh\nbin/mapred --daemon start historyserver\n")),(0,r.kt)("blockquote",null,(0,r.kt)("ul",{parentName:"blockquote"},(0,r.kt)("li",{parentName:"ul"},"HDFS \u754c\u9762\uff1a",(0,r.kt)("a",{parentName:"li",href:"http://node101:9870"},"http://node101:9870"),"\u3001",(0,r.kt)("a",{parentName:"li",href:"http://node101:9868"},"http://node101:9868")),(0,r.kt)("li",{parentName:"ul"},"YARN \u754c\u9762\uff1a",(0,r.kt)("a",{parentName:"li",href:"http://node101:8088"},"http://node101:8088")))),(0,r.kt)("h3",{id:"fully-distributed-\u6a21\u5f0f"},"Fully-Distributed \u6a21\u5f0f"),(0,r.kt)("h4",{id:"\u96c6\u7fa4\u89c4\u5212"},"\u96c6\u7fa4\u89c4\u5212"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"hostname"),(0,r.kt)("th",{parentName:"tr",align:null},"HDFS"),(0,r.kt)("th",{parentName:"tr",align:null},"YARN"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"node101"),(0,r.kt)("td",{parentName:"tr",align:null},"DataNode\u3001NameNode"),(0,r.kt)("td",{parentName:"tr",align:null},"NodeManager")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"node102"),(0,r.kt)("td",{parentName:"tr",align:null},"DataNode"),(0,r.kt)("td",{parentName:"tr",align:null},"NodeManager\u3001ResourceManager\u3001JobHistoryServer")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"node103"),(0,r.kt)("td",{parentName:"tr",align:null},"DataNode\u3001SecondaryNameNode"),(0,r.kt)("td",{parentName:"tr",align:null},"NodeManager")))),(0,r.kt)("h4",{id:"\u914d\u7f6e\u6587\u4ef6-1"},"\u914d\u7f6e\u6587\u4ef6"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/hadoop-env.sh")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},"\u4e0e Preudo-Distributed \u6a21\u5f0f\u76f8\u540c"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/workers")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"node101\nnode102\nnode103\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/core-site.xml")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},"\u4e0e Preudo-Distributed \u6a21\u5f0f\u76f8\u540c"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/hdfs-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>dfs.client.use.datanode.hostname</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>dfs.namenode.http-address</name>\n        <value>node101:9870</value>\n    </property>\n    <property>\n        <name>dfs.namenode.secondary.http-address</name>\n        <value>node103:9868</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/yarn-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <value>node102</value>\n    </property>\n    <property>\n        <name>yarn.log-aggregation-enable</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>yarn.log.server.url</name>\n        <value>http://node102:19888/jobhistory/logs</value>\n    </property>\n    <property>\n        <name>yarn.log-aggregation.retain-seconds</name>\n        <value>604800</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/mapred-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n       <property>\n        <name>yarn.app.mapreduce.am.env</name>\n        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n    </property>\n    <property>\n        <name>mapreduce.map.env</name>\n        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n    </property>\n    <property>\n        <name>mapreduce.reduce.env</name>\n        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n    </property>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n    <property>\n        <name>mapreduce.jobhistory.address</name>\n        <value>node102:10020</value>\n    </property>\n    <property>\n        <name>mapreduce.jobhistory.webapp.address</name>\n        <value>node102:19888</value>\n    </property>\n</configuration>\n')))),(0,r.kt)("h4",{id:"\u5206\u53d1\u914d\u7f6e"},"\u5206\u53d1\u914d\u7f6e"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# node101\nrsync -a -r -v $HADOOP_HOME node102:/opt\nrsync -a -r -v $HADOOP_HOME node103:/opt\n")),(0,r.kt)("h4",{id:"\u683c\u5f0f\u5316-namenode-1"},"\u683c\u5f0f\u5316 NameNode"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# node101\ncd $HADOOP_HOMOE\nbin/hdfs namenode -format\n")),(0,r.kt)("h4",{id:"\u542f\u52a8-1"},"\u542f\u52a8"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd $HADOOP_HOME\n# node101\nsbin/start-dfs.sh\n# node102\nsbin/start-yarn.sh\n# node102\nbin/mapred --daemon start historyserver\n")),(0,r.kt)("h4",{id:"\u9a8c\u8bc1"},"\u9a8c\u8bc1"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u5feb\u6377\u811a\u672c ",(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/run.sh")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/bin/bash\ncase $1 in\nstart)\n    ssh node101 "${HADOOP_HOME}/sbin/start-dfs.sh"\n    ssh node102 "${HADOOP_HOME}/sbin/start-yarn.sh"\n    ssh node102 "${HADOOP_HOME}/sbin/mapred --daemon start historyserver"\n;;\nstop)\n    ssh node101 "${HADOOP_HOME}/sbin/stop-dfs.sh"\n    ssh node102 "${HADOOP_HOME}/sbin/stop-yarn.sh"\n    ssh node102 "${HADOOP_HOME}/sbin/mapred stop historyserver"\n;;\nrsync)\n    for hostname in node101 node102 node103; do\n        if [[ "$(hostname)" != "${hostname}" ]]; then\n            echo "========== ${hostname} =========="\n            rsync -a -r -v "${HADOOP_HOME}" "${hostname}:/opt"\n        fi\n    done\n;;\njps)\n    for hostname in node101 node102 node103; do\n        echo "========== ${hostname} =========="\n        ssh "${hostname}" jps -mlvV\n    done\n;;\n*)\n    echo \'USAGE: run.sh <start | stop | rsync | jps>\'\nesac\n')),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"chmod +x run.sh\n./run.sh jps\n")))),(0,r.kt)("h3",{id:"high-availability-\u6a21\u5f0f"},"High-Availability \u6a21\u5f0f"),(0,r.kt)("h4",{id:"\u96c6\u7fa4\u89c4\u5212-1"},"\u96c6\u7fa4\u89c4\u5212"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"hostname"),(0,r.kt)("th",{parentName:"tr",align:null},"HDFS"),(0,r.kt)("th",{parentName:"tr",align:null},"YARN"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"node101"),(0,r.kt)("td",{parentName:"tr",align:null},"DataNode\u3001NameNode"),(0,r.kt)("td",{parentName:"tr",align:null},"NodeManager\u3001ResourceManager")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"node102"),(0,r.kt)("td",{parentName:"tr",align:null},"DataNode\u3001NameNode"),(0,r.kt)("td",{parentName:"tr",align:null},"NodeManager\u3001ResourceManager\u3001JobHistoryServer")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"node103"),(0,r.kt)("td",{parentName:"tr",align:null},"DataNode\u3001NameNode"),(0,r.kt)("td",{parentName:"tr",align:null},"NodeManager\u3001ResourceManager")))),(0,r.kt)("h4",{id:"\u914d\u7f6e\u6587\u4ef6-2"},"\u914d\u7f6e\u6587\u4ef6"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/hadoop-env.sh")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},"HA \u6a21\u5f0f\u4e0d\u9700\u8981 SecondaryNameNode")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"export JAVA_HOME=/opt/jdk-8\nexport HADOOP_HOME=/opt/hadoop-3.3.1\nexport HDFS_NAMENODE_USER=root\nexport HDFS_DATANODE_USER=root\nexport HDFS_JOURNALNODE_USER=root\nexport HDFS_ZKFC_USER=root\nexport YARN_RESOURCEMANAGER_USER=root\nexport YARN_NODEMANAGER_USER=root\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/workers")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},"\u4e0e Fully-Distributed \u6a21\u5f0f\u76f8\u540c"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/core-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://hdfs-cluster</value>\n    </property>\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>/opt/hadoop-3.3.1/data</value>\n    </property>\n    <property>\n        <name>hadoop.http.staticuser.user</name>\n        <value>root</value>\n    </property>\n    <property>\n        <name>hadoop.proxyuser.root.hosts</name>\n        <value>*</value>\n    </property>\n    <property>\n        <name>hadoop.proxyuser.root.groups</name>\n        <value>*</value>\n    </property>\n\n    \x3c!-- HDFS Zookeeper \u5730\u5740 --\x3e\n    <property>\n        <name>ha.zookeeper.quorum</name>\n        <value>win10:2181,win10:8182,win10:2183</value>\n    </property>\n\n    \x3c!-- YARN Zookeeper \u5730\u5740 --\x3e\n    <property>\n        <name>hadoop.zk.address</name>\n        <value>win10:2181,win10:2182,win10:2183</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/hdfs-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>dfs.client.use.datanode.hostname</name>\n        <value>true</value>\n    </property>\n    \x3c!-- HDFS NN HA --\x3e\n    <property>\n        <name>dfs.nameservices</name>\n        <value>hdfs-cluster</value>\n    </property>\n    <property>\n        <name>dfs.ha.namenodes.hdfs-cluster</name>\n        <value>nn1, nn2, nn3</value>\n    </property>\n\n    \x3c!-- HDFS NN rpc --\x3e\n    <property>\n        <name>dfs.namenode.rpc-address.hdfs-cluster.nn1</name>\n        <value>node101:8020</value>\n    </property>\n    <property>\n        <name>dfs.namenode.rpc-address.hdfs-cluster.nn2</name>\n        <value>node102:8020</value>\n    </property>\n    <property>\n        <name>dfs.namenode.rpc-address.hdfs-cluster.nn3</name>\n        <value>node103:8020</value>\n    </property>\n\n    \x3c!-- HDFS NN http --\x3e\n    <property>\n        <name>dfs.namenode.http-address.hdfs-cluster.nn1</name>\n        <value>node101:9870</value>\n    </property>\n    <property>\n        <name>dfs.namenode.http-address.hdfs-cluster.nn2</name>\n        <value>node102:9870</value>\n    </property>\n    <property>\n        <name>dfs.namenode.http-address.hdfs-cluster.nn3</name>\n        <value>node103:9870</value>\n    </property>\n\n    \x3c!-- HDFS JournalNode --\x3e\n    <property>\n        <name>dfs.namenode.shared.edits.dir</name>\n        <value>qjournal://node101:8485;node102:8485;node103:8485/hdfs-cluster</value>\n    </property>\n    <property>\n        <name>dfs.journalnode.edits.dir</name>\n        <value>/opt/hadoop-3.3.1/data/dfs/journalnode/</value>\n    </property>\n\n    \x3c!-- HDFS fencing --\x3e\n    <property>\n        <name>dfs.ha.fencing.methods</name>\n        <value>\n            sshfence\n            shell(/bin/true)\n        </value>\n    </property>\n    <property>\n        <name>dfs.ha.fencing.ssh.private-key-files</name>\n        <value>/root/.ssh/id_rsa</value>\n    </property>\n\n    \x3c!-- HDFS automatic failover --\x3e\n    <property>\n        <name>dfs.ha.automatic-failover.enabled</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>dfs.client.failover.proxy.provider.hdfs-cluster</name>\n        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/yarn-site.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},'<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n\n    \x3c!-- YARN RM HA --\x3e\n    <property>\n        <name>yarn.resourcemanager.ha.enabled</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.cluster-id</name>\n        <value>yarn-cluster</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.ha.rm-ids</name>\n        <value>rm1, rm2, rm3</value>\n    </property>\n\n    \x3c!-- YARN RM --\x3e\n    <property>\n        <name>yarn.resourcemanager.hostname.rm1</name>\n        <value>node101</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.hostname.rm2</name>\n        <value>node102</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.hostname.rm3</name>\n        <value>node103</value>\n    </property>\n\n    \x3c!-- YARN RM http --\x3e\n    <property>\n        <name>yarn.resourcemanager.webapp.address.rm1</name>\n        <value>node101:8088</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.webapp.address.rm2</name>\n        <value>node102:8088</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.webapp.address.rm3</name>\n        <value>node103:8088</value>\n    </property>\n\n    \x3c!-- YARN RM recovery --\x3e\n    <property>\n        <name>yarn.resourcemanager.recovery.enabled</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>yarn.resourcemanager.store.class</name>\n        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n    </property>\n</configuration>\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"$HADOOP_HOME/etc/hadoop/mapred-site.xml")))),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"\u4e0e Full-Distributed \u6a21\u5f0f\u76f8\u540c")),(0,r.kt)("h4",{id:"\u5206\u53d1\u914d\u7f6e-1"},"\u5206\u53d1\u914d\u7f6e"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"\u4e0e Full-Distributed \u6a21\u5f0f\u76f8\u540c")),(0,r.kt)("h4",{id:"\u542f\u52a8-journalnode"},"\u542f\u52a8 JournalNode"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd $HADOOP_HOME\n# node101 | node102 | node103\nbin/hdfs --daemon start journalnode\n")),(0,r.kt)("h4",{id:"\u683c\u5f0f\u5316-namenode-2"},"\u683c\u5f0f\u5316 NameNode"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# node101\nbin/hdfs namenode -format\n# node102 | node103\nbin/hdfs namenode -bootstrapStandby\n")),(0,r.kt)("h4",{id:"\u683c\u5f0f\u5316-zk"},"\u683c\u5f0f\u5316 ZK"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# node102\nsbin/hdfs zkfs -formatZK\n")),(0,r.kt)("h4",{id:"\u542f\u52a8-2"},"\u542f\u52a8"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sbin/start-dfs.sh\nsbin/start-yarn.sh\n\n# node102\nbin/mapred --daemon start historyserver\n")),(0,r.kt)("h4",{id:"ha-\u72b6\u6001"},"HA \u72b6\u6001"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# \u67e5\u770b\nbin/hdfs haadmin -getAllServiceState\nbin/yarn rmadmin -getAllServiceState\n\n# \u624b\u52a8\u5207\u6362\nbin/hdfs haadmin -transitionToActive --forceactive\nbin/yarn rmadmin -transitionToActive --forceactive\n")),(0,r.kt)("h2",{id:"wordcount"},"WordCount"),(0,r.kt)("h4",{id:"\u793a\u4f8b"},"\u793a\u4f8b"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u6587\u672c"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd $HADOOP_HOME\n\nmkdir -p input\n\ncat > input/word.txt <<- EOF\ni keep saying no\nthis can not be the way it was supposed to be\ni keep saying no\nthere has gotta be a way to get you close to me\nEOF\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u4e0a\u4f20"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"bin/hadoop fs -mkdir /input\n\nbin/hadoop fs -put input/word.txt /input\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u6267\u884c"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input /output\n")))),(0,r.kt)("h3",{id:"hdfs-\u5ba2\u6237\u7aef"},"HDFS \u5ba2\u6237\u7aef"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"winutils")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://github.com/kontext-tech/winutils/blob/master/hadoop-3.3.1/bin/winutils.exe\n\nmkdir -p /d/env/hadoop-3.3.1/bin\n\nmv winutils.exe /d/env/hadoop-3.3.1/bin\n")),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},"\u914d\u7f6e ",(0,r.kt)("inlineCode",{parentName:"p"},"HADOOP_HOME")," \u5230\u73af\u5883\u53d8\u91cf"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"pom.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-client</artifactId>\n    <version>3.3.1</version>\n</dependency>\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u5728\u5ba2\u6237\u7aef\u6a21\u62df Hadoop \u96c6\u7fa4 DataNode \u7684 ",(0,r.kt)("inlineCode",{parentName:"p"},"hostname")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"echo '192.168.137.101 node101' >> /c/Windows/System32/drivers/etc/hosts\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u4e0b\u8f7d\u6587\u4ef6"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-java"},'public class Main {\n    public static final String CLASSPATH = Main.class.getResource("/").toString();\n\n    public static void main(String[] args) throws IOException, InterruptedException {\n        Configuration conf = new Configuration();\n        conf.set("dfs.client.use.datanode.hostname", "true");\n        FileSystem fs = FileSystem.get(URI.create("hdfs://node101:9000"), conf, "root");\n        fs.copyToLocalFile(new Path("/input/word.txt"), new Path(CLASSPATH + "/word.txt"));\n        fs.close();\n    }\n}\n')))),(0,r.kt)("h3",{id:"mapreduce-\u7a0b\u5e8f"},"MapReduce \u7a0b\u5e8f"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"pom.xml")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-client</artifactId>\n    <version>3.3.1</version>\n</dependency>\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<plugin>\n    <artifactId>maven-assembly-plugin</artifactId>\n    <configuration>\n        <descriptorRefs>\n            <descriptorRef>jar-with-dependencies</descriptorRef>\n        </descriptorRefs>\n    </configuration>\n    <executions>\n        <execution>\n            <id>make-assembly</id>\n            <phase>package</phase>\n            <goals>\n                <goal>single</goal>\n            </goals>\n        </execution>\n    </executions>\n</plugin>\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"WordCount"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-java"},'public class WordCount {\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(WordCount.class);\n\n        job.setMapperClass(WordCount.WordCountMapper.class);\n        job.setReducerClass(WordCount.WordCountReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean result = job.waitForCompletion(true);\n        System.exit(result ? 0 : 1);\n    }\n\n    public static class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            String line = value.toString();\n            String[] words = line.split(" ");\n            for (String word : words) {\n                context.write(new Text(word), new IntWritable(1));\n            }\n        }\n    }\n\n    public static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int count = 0;\n            for (IntWritable value : values) {\n                count += value.get();\n            }\n            context.write(key, new IntWritable(count));\n        }\n    }\n}\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u6253\u5305"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"mvn clean package\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u6267\u884c"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd $HADOOP_HOME\n\nbin/hadoop jar word-count-0.0.1-jar-with-dependencies.jar xyz.icefery.demo.WordCount /input /output\n\nbin/hadoop fs -get /output/part-r-00000 result.txt\n\ncat result.txt\n")))))}m.isMDXComponent=!0}}]);